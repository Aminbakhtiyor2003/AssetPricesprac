# -*- coding: utf-8 -*-
"""100yeardataplayground.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ddlz4OBEmihPxx93Uq_5MOwj-Vgdg0Rx

"""

import pandas as pd

# Replace 'your_excel_file.xlsx' with the actual path to your Excel file
excel_file_path = 'Century of Factor Premia Monthly.xlsx'

# Replace 'SheetName' with the actual name of the sheet you want to access
sheet_name_to_access = 'Data'

try:
  # Read the specified sheet from the Excel file into a pandas DataFrame
  df = pd.read_excel(excel_file_path, sheet_name=sheet_name_to_access)

  # Display the first few rows of the DataFrame
  display(df.head())

except FileNotFoundError:
  print(f"Error: The file '{excel_file_path}' was not found.")
except Exception as e:
  print(f"An error occurred: {e}")

import pandas as pd

# Step 1: Load your Excel file and target sheet
excel_file_path = 'Century of Factor Premia Monthly.xlsx'
sheet_name = 'Data'
df = pd.read_excel(excel_file_path, sheet_name=sheet_name)

# Step 2: Rename the unnamed first column to 'Date'
df = df.rename(columns={'Unnamed: 0': 'Date'})

# Step 3: Convert to datetime (handles weird formats robustly)
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Step 4: Set 'Date' as the index and sort the DataFrame by date
df.set_index('Date', inplace=True)
df.sort_index(inplace=True)

# Step 5: Drop columns that are entirely empty
df.dropna(axis=1, how='all', inplace=True)

# Step 6: (Optional) Filter to start from a specific date
filter_start = pd.Timestamp('1980-01-01')
df_aligned = df[df.index >= filter_start]

# Step 7: Drop remaining NaNs to ensure clean data
df_aligned.dropna(axis=1, how='any', inplace=True)
df_aligned.dropna(axis=0, how='any', inplace=True)

# Step 8: Validation checks
print("Date range:", df_aligned.index.min(), "→", df_aligned.index.max())
print("Shape:", df_aligned.shape)
display(df_aligned.head())

"""1970s Data"""



import numpy as np

if 'df' in locals():
    numerical_cols = df.select_dtypes(include=['number']).columns

    # Arithmetic Mean (as percent)
    arithmetic_means = (df[numerical_cols].mean() *100).rename("Arithmetic Means %")

    def safe_geometric_mean(series):
        # Use the entire series from the beginning
        if (series + 1 <= 0).any() or len(series) == 0:
            return np.nan
        # Ensure the product is not zero or negative before taking the power
        product = np.prod(1 + series)
        if product <= 0:
            return np.nan
        return (product)**(1 / len(series)) - 1

    geometric_means = df[numerical_cols].apply(safe_geometric_mean) * 100
    geometric_means = geometric_means.rename("Geometric Means %")
    #so esentially how much it compounds per year

    print("Arithmetic Means (%):")
    display(arithmetic_means.round(2))

    print("\nGeometric Means (%):")
    display(geometric_means.round(2))

else:
    print("DataFrame 'df' not found. Please load the data first.")

if 'arithmetic_means' in locals() and 'geometric_means' in locals():
    print("Top 5 Highest Arithmetic Means (%):")
    display(arithmetic_means.nlargest(5))
    display(arithmetic_means.nsmallest(5))

    print("\nTop 5 Highest Geometric Means (%):")
    display(geometric_means.nlargest(5))
    display(geometric_means.nsmallest(5))
else:
    print("Arithmetic means and/or Geometric means not found. Please run the previous cells to calculate them.")

"""# Ensure the first column is the date column
# Use the actual name of the first column from the dataframe head output
df = df.rename(columns={df.columns[0]: 'Date'})

# Convert the 'Date' column to datetime objects
df['Date'] = pd.to_datetime(df['Date'])

# Set the 'Date' column as the index
df.set_index('Date', inplace=True)

# Step 1: Drop columns that are entirely NaN (just in case)
df.dropna(axis=1, how='all', inplace=True)

# Step 2: Find the latest first valid date across all columns
start_dates = df.apply(lambda col: col.first_valid_index())
latest_start = start_dates.max()

# Step 3: Trim all columns to start from the same latest start date
df_aligned = df[df.index >= latest_start]

# Step 4: Drop columns that still have NaNs (if any didn't have full coverage)
df_aligned = df_aligned.dropna(axis=1, how='any')

# Optional: Display aligned data shape and range
print(f"Aligned start date: {latest_start}")
print(f"Aligned shape: {df_aligned.shape}")
display(df_aligned.head())

# Top 5 and Bottom 5 Geo and Art means
"""

if 'arithmetic_means' in locals() and 'geometric_means' in locals():
    print("Top 5 Highest Arithmetic Means (%):")
    display(arithmetic_means.nlargest(5))
    print("\nBottom 5 Lowest Arithmetic Means (%):")
    display(arithmetic_means.nsmallest(5))


    print("\nTop 5 Highest Geometric Means (%):")
    display(geometric_means.nlargest(5))
    print("\nBottom 5 Lowest Geometric Means (%):")
    display(geometric_means.nsmallest(5))
else:
    print("Arithmetic means and/or Geometric means not found. Please run the previous cells to calculate them.")

"""# Sharpe Ratios and Vol (STD)"""

# Annualized standard deviation (volatility)
volatility = df[numerical_cols].std() * np.sqrt(12) * 100  # Convert to %
volatility = volatility.rename("Volatility %")

annual_rf = 0.02  # 2%
monthly_rf = (1 + annual_rf)**(1/12) - 1  # ~0.165%

# Adjust Sharpe Ratio
sharpe_ratio = ((df[numerical_cols].mean() - monthly_rf) / df[numerical_cols].std()) * np.sqrt(12)
sharpe_ratio = sharpe_ratio.rename("Sharpe Ratio")

# Combine into one summary DataFrame
summary_df = pd.concat([arithmetic_means, geometric_means, volatility, sharpe_ratio], axis=1)

# Sort and display
print("Summary: Arithmetic Mean, Geometric Mean, Volatility, Sharpe Ratio")
display(summary_df.sort_values(by="Sharpe Ratio", ascending=False).round(2))

from scipy.stats import skew, kurtosis

# Calculate skewness and excess kurtosis
skewness = df_aligned.apply(skew)
kurt = df_aligned.apply(lambda x: kurtosis(x, fisher=True))  # Excess kurtosis

# Combine results into a summary DataFrame
summary_stats = pd.DataFrame({
    'Skewness': skewness,
    'Excess Kurtosis': kurt
})

# Display results sorted by tail risk (optional)
summary_stats = summary_stats.sort_values(by='Excess Kurtosis', ascending=False)
display(summary_stats.round(3))

"""# Sharpe Ratio Through Time

# Kurtosis and Skewness
"""

from scipy.stats import skew, kurtosis

# Calculate skewness and excess kurtosis
skewness = df_aligned.apply(skew)
kurt = df_aligned.apply(lambda x: kurtosis(x, fisher=True))  # Excess kurtosis

# Combine results into a summary DataFrame
summary_stats = pd.DataFrame({
    'Skewness': skewness,
    'Excess Kurtosis': kurt
})

# Display results sorted by tail risk (optional)
summary_stats = summary_stats.sort_values(by='Excess Kurtosis', ascending=False)
display(summary_stats.round(3))

"""#GMM model and PCA, ML techniques

"""

import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# Use your cleaned DataFrame (df)
print("Data shape:", df.shape)

# --- Select key factors manually ---
selected_factors = [
    'US Stock Selection Multi-style',
    'Fixed income Carry',
    'All asset classes Momentum',
    'Equity indices Market'
]

# Validate factor presence
for col in selected_factors:
    if col not in df.columns:
        raise ValueError(f"Missing column: {col}")

X = df[selected_factors].copy()

# --- Fit GMM ---
n_components = 6
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm.fit(X)

# --- Predict regimes ---
regimes = gmm.predict(X)
df_with_regimes = df.copy()
df_with_regimes['Regime'] = regimes

# --- Plot regimes over time ---
plt.figure(figsize=(14, 5))
plt.plot(df_with_regimes.index, df_with_regimes['Regime'], linestyle='-', marker='o', alpha=0.6)
plt.title('GMM Regimes Over Time (Selected Factors)')
plt.ylabel('Regime')
plt.xlabel('Date')
plt.grid(True)
plt.show()

# --- Show regime counts ---
print(df_with_regimes['Regime'].value_counts().sort_index())

import pandas as pd
import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Use your cleaned DataFrame (df)
print("Data shape:", df.shape)

# --- Drop missing values (to ensure PCA works) ---
df_pca = df.dropna(axis=1, how='any').copy()
if 'Regime' in df_pca.columns:
    df_pca.drop(columns=['Regime'], inplace=True)

# --- Run PCA ---
pca = PCA(n_components=3)
X_pca = pd.DataFrame(pca.fit_transform(df_pca), index=df_pca.index)
print("Explained variance by PCA:", pca.explained_variance_ratio_)

# --- Fit GMM ---
n_components = 3
gmm = GaussianMixture(n_components=n_components, covariance_type='full', random_state=42)
gmm.fit(X_pca)

# --- Predict regimes ---
regimes = gmm.predict(X_pca)
df_with_regimes = df_pca.copy()
df_with_regimes['Regime'] = regimes

# --- Plot regimes over time ---
plt.figure(figsize=(14, 5))
plt.plot(df_with_regimes.index, df_with_regimes['Regime'], linestyle='-', marker='o', alpha=0.6)
plt.title('GMM Regimes Over Time (PCA on All Factors)')
plt.ylabel('Regime')
plt.xlabel('Date')
plt.grid(True)
plt.show()

# --- Show regime counts ---
print(df_with_regimes['Regime'].value_counts().sort_index())

from sklearn.mixture import GaussianMixture

bics = []
ks = range(2, 10)
for k in ks:
    gmm = GaussianMixture(n_components=k, random_state=42).fit(X_pca)
    bics.append(gmm.bic(X_pca))

plt.plot(ks, bics, marker='o')
plt.title('BIC for Optimal Regime Count')
plt.xlabel('Number of Regimes')
plt.ylabel('BIC Score')
plt.grid(True)
plt.show()

"""# Max Drawdown & Drawdown Frequency

"""

def calculate_drawdown(series):
    """Calculates the drawdown of a time series."""
    cumulative_returns = (1 + series).cumprod()
    peak = cumulative_returns.expanding().max()
    drawdown = (cumulative_returns - peak) / peak
    return drawdown

def calculate_max_drawdown(series):
    """Calculates the maximum drawdown of a time series."""
    return calculate_drawdown(series).min() * 100

def calculate_drawdown_frequency(series):
    """Calculates the frequency of drawdowns (months with negative returns)."""
    return (series < 0).sum() / len(series) * 100

# Calculate Max Drawdown and Drawdown Frequency for each column
max_drawdowns = df.apply(calculate_max_drawdown).rename("Max Drawdown %")
drawdown_frequencies = df.apply(calculate_drawdown_frequency).rename("Drawdown Frequency %")

# Combine into a summary DataFrame
drawdown_summary_df = pd.concat([max_drawdowns, drawdown_frequencies], axis=1)

# Display results
print("Drawdown Summary:")
display(drawdown_summary_df.sort_values(by="Max Drawdown %").round(2))

import pandas as pd

# --- Helper: Max Drawdown ---
def calculate_drawdown(series):
    """Calculates drawdown series from return series."""
    cumulative = (1 + series).cumprod()
    peak = cumulative.expanding().max()
    drawdown = (cumulative - peak) / peak
    return drawdown

def calculate_max_drawdown(series):
    """Returns max drawdown in percentage."""
    drawdown = calculate_drawdown(series)
    return drawdown.min() * 100

# --- Helper: Drawdown Frequency ---
def calculate_drawdown_frequency(series):
    """Counts the number of distinct drawdown events."""
    cumulative = (1 + series).cumprod()
    peak = cumulative.expanding().max()
    in_drawdown = cumulative < peak

    # A new drawdown starts when the series enters a drawdown
    drawdown_starts = (~in_drawdown.shift(1, fill_value=False)) & in_drawdown
    return drawdown_starts.sum()

# --- Apply to each factor column ---
max_drawdowns = df.apply(calculate_max_drawdown).rename("Max Drawdown (%)")
drawdown_frequencies = df.apply(calculate_drawdown_frequency).rename("Drawdown Events")

# --- Combine summary ---
summary_df = pd.concat([max_drawdowns, drawdown_frequencies], axis=1)
summary_df = summary_df.sort_values(by="Drawdown Events", ascending=False)


summary_df_two= summary_df.sort_values(by="Max Drawdown (%)")

# --- Display ---
print("Drawdown Summary:")
display(summary_df.round(2))
display(summary_df_two.round(2))

"""# Recovery Times"""

import pandas as pd

# --- Max Drawdown ---
def calculate_drawdown(series):
    """Calculates drawdown series from return series."""
    cumulative = (1 + series).cumprod()
    peak = cumulative.expanding().max()
    drawdown = (cumulative - peak) / peak
    return drawdown

def calculate_max_drawdown(series):
    """Returns max drawdown in percentage."""
    drawdown = calculate_drawdown(series)
    return drawdown.min() * 100

# --- Drawdown Frequency ---
def calculate_drawdown_frequency(series):
    """Calculates the number of distinct drawdown events."""
    cumulative = (1 + series).cumprod()
    peak = cumulative.expanding().max()
    in_drawdown = cumulative < peak

    # A new drawdown starts when the series enters a drawdown
    drawdown_starts = (~in_drawdown.shift(1, fill_value=False)) & in_drawdown
    return drawdown_starts.sum()

def calculate_recovery_times(series):
    """Returns a list of recovery times (in periods) from true peak to recovery."""
    series = series.dropna()
    if len(series) < 2:
        return []

    cumulative = (1 + series).cumprod()

    recovery_times = []
    in_drawdown = False
    peak_value = cumulative.iloc[0]
    peak_index = 0

    for i in range(1, len(cumulative)):
        current = cumulative.iloc[i]

        if in_drawdown:
            if current >= peak_value * 0.9999:
                # Recovered to within 0.01% of prior peak
                recovery_time = i - peak_index
                recovery_times.append(recovery_time)
                in_drawdown = False
                peak_value = current
                peak_index = i
        else:
            if current > peak_value:
                peak_value = current
                peak_index = i
            elif current < peak_value:
                in_drawdown = True

    #print(f"{series.name}: Found {len(recovery_times)} recovery events. "
          #f"Avg: {sum(recovery_times)/len(recovery_times) if recovery_times else 'N/A'}")

    return recovery_times

# --- Apply to DataFrame ---
# Assume df is your clean DataFrame with monthly return data (index = datetime)
# df = ...

# 1. Max drawdown
max_drawdowns = df.apply(calculate_max_drawdown).rename("Max Drawdown (%)")

# 2. Drawdown event frequency
drawdown_frequencies = df.apply(calculate_drawdown_frequency).rename("Drawdown Events")

# 3. Recovery stats
recovery_stats = {}
for col in df.columns:
    times = calculate_recovery_times(df[col])
    if times:
        recovery_stats[col] = {
            "Avg Recovery Time": sum(times) / len(times),
            "Max Recovery Time": max(times),
            "Min Recovery Time": min(times),
        }
    else:
        recovery_stats[col] = {
            "Avg Recovery Time": None,
            "Max Recovery Time": None,
            "Min Recovery Time": None,
        }

# Convert recovery stats to DataFrame
recovery_df = pd.DataFrame(recovery_stats).T

# Combine all into one summary table
summary_df = pd.concat([max_drawdowns, drawdown_frequencies, recovery_df], axis=1)


# --- Display ---
print("Drawdown and Recovery Summary:")
display(summary_df.round(2))
display(summary_stats)



"""# Target Month"""

print(df.index.unique())

# Choose your target month
target_month = pd.Timestamp('2024-06-28')  # change this as needed

# Ensure 'Date' is the index
if 'Date' in df.columns:
    df.set_index('Date', inplace=True)

# Check if the target month is in the index
if target_month in df.index:
    # Extract the row as a 1-row DataFrame (preserves column names)
    target_row_df = df.loc[[target_month]]  # Double brackets return DataFrame, not Series
    print(f"Return vector for {target_month.strftime('%Y-%m-%d')}:")
    display(target_row_df)
else:
    print(f"Error: Target month '{target_month.strftime('%Y-%m-%d')}' not found in the DataFrame index.")

"""# Comparison"""



"""Eucliean Distance"""

from sklearn.metrics.pairwise import euclidean_distances

# Step 1: Drop the target month from df_aligned
print(f"\n[DEBUG] Full df_aligned shape: {df_aligned.shape}")
print(f"[DEBUG] Dropping target month: {target_month}")
comparison_df = df_aligned.drop(index=target_month, errors='ignore')
print(f"[DEBUG] comparison_df shape after dropping target: {comparison_df.shape}")

# Step 2: Drop rows with NaNs (but keep all columns)
nan_rows = comparison_df[comparison_df.isna().any(axis=1)]
print(f"\n[DEBUG] Number of rows with NaNs being dropped: {nan_rows.shape[0]}")
if not nan_rows.empty:
    print("[DEBUG] Example rows with NaNs:")
    display(nan_rows.head())

comparison_df_cleaned = comparison_df.dropna(axis=0, how='any')
print(f"[DEBUG] comparison_df_cleaned shape (after dropping NaN rows): {comparison_df_cleaned.shape}")

# Step 3: Prepare target row
print(f"\n[DEBUG] Columns in df_aligned: {df_aligned.columns.tolist()[:5]} ...")
print(f"[DEBUG] Aligning target row to comparison_df_cleaned columns")

# Ensure target row includes all columns in same order
target_row_df_cleaned = target_row_df[df_aligned.columns]
target_row_df_cleaned = target_row_df_cleaned[comparison_df_cleaned.columns]

print(f"[DEBUG] target_row_df_cleaned shape: {target_row_df_cleaned.shape}")
print("[DEBUG] Target month return vector:")
display(target_row_df_cleaned)

# Step 4: Convert to NumPy
target_vector_cleaned = target_row_df_cleaned.values
comparison_matrix_cleaned = comparison_df_cleaned.values
print(f"\n[DEBUG] target_vector_cleaned shape: {target_vector_cleaned.shape}")
print(f"[DEBUG] comparison_matrix_cleaned shape: {comparison_matrix_cleaned.shape}")

# Step 5: Compute Euclidean distances
distances = euclidean_distances(target_vector_cleaned, comparison_matrix_cleaned)[0]
print(f"[DEBUG] Computed distances for {len(distances)} months.")

# Step 6: Build distance DataFrame
distance_df = pd.DataFrame({'Distance': distances}, index=comparison_df_cleaned.index)
print(f"[DEBUG] distance_df shape: {distance_df.shape}")

# Step 7: Display top similar months
print(f"\n📌 Months most similar to {target_month.strftime('%Y-%m-%d')} (based on Euclidean Distance):")
display(distance_df.sort_values(by='Distance').head())

"""Cosine Distance"""

from sklearn.metrics.pairwise import cosine_similarity

# Step 1: Drop the target month
print(f"\n[DEBUG] Full df_aligned shape: {df_aligned.shape}")
print(f"[DEBUG] Dropping target month: {target_month}")
comparison_df = df_aligned.drop(index=target_month, errors='ignore')
print(f"[DEBUG] comparison_df shape after dropping target: {comparison_df.shape}")

# Step 2: Drop rows with any NaNs (keep all columns)
nan_rows = comparison_df[comparison_df.isna().any(axis=1)]
print(f"\n[DEBUG] Rows with NaNs being dropped: {nan_rows.shape[0]}")
comparison_df_cleaned = comparison_df.dropna(axis=0, how='any')
print(f"[DEBUG] comparison_df_cleaned shape: {comparison_df_cleaned.shape}")

# Step 3: Prepare target month vector
target_row_df_cleaned = target_row_df[df_aligned.columns]
target_row_df_cleaned = target_row_df_cleaned[comparison_df_cleaned.columns]


print(f"\n[DEBUG] target_row_df_cleaned shape: {target_row_df_cleaned.shape}")
display(target_row_df_cleaned)

# Step 4: Convert to numpy
target_vector = target_row_df_cleaned.values
comparison_matrix = comparison_df_cleaned.values

# Step 5: Compute cosine similarity
# Reshape target_vector to be 2D for cosine_similarity
cosine_sim = cosine_similarity(target_vector.reshape(1, -1), comparison_matrix)[0]

# Step 6: Build similarity DataFrame
similarity_df = pd.DataFrame({'Cosine Similarity': cosine_sim}, index=comparison_df_cleaned.index)

# Sort by similarity (higher is more similar) and display the top similar months
print(f"\n📌 Months most similar to {target_month.strftime('%Y-%m-%d')} (based on Cosine Similarity):")
display(similarity_df.sort_values(by='Cosine Similarity', ascending=False).head())

import pandas as pd

# Step 7: Select top N similar months
top_n = 5
top_matches = similarity_df.sort_values(by='Cosine Similarity', ascending=False).head(top_n)
top_match_dates = top_matches.index.tolist()

# Include the target month at the top
all_selected_dates = [target_month] + top_match_dates

# Step 8: Subset the data for these dates
df_vectors_to_display = df_aligned.loc[all_selected_dates]

# Optional: Format the index nicely
df_vectors_to_display.index = df_vectors_to_display.index.strftime('%Y-%m-%d')

# Optional: round values for display
df_vectors_to_display_rounded = df_vectors_to_display.round(4)

# Step 9: Transpose for easy comparison (Assets as rows, Dates as columns)
comparison_table = df_vectors_to_display_rounded.T

# Step 10: Display
import IPython
IPython.display.display(comparison_table)

"""#Directional Pattern"""

# Step 11: Label each return value as 'Positive', 'Negative', or 'Flat'

threshold = 0.002  # 0.2% threshold

def label_return(x):
    if x > threshold:
        return 'Positive'
    elif x < -threshold:
        return 'Negative'
    else:
        return 'Flat'

# Apply label function to each return value
comparison_labels = df_vectors_to_display.applymap(label_return)  # Dates as rows, Assets as columns

# Step 12: Transpose so assets are rows
comparison_labels_t = comparison_labels.T  # Now rows = assets, columns = dates

# Step 13: Calculate % Positive / Flat / Negative for each asset (row-wise)
def summarize_row(row):
    counts = row.value_counts(normalize=True)
    return pd.Series({
        '% Positive': round(counts.get('Positive', 0) * 100, 1),
        '% Flat': round(counts.get('Flat', 0) * 100, 1),
        '% Negative': round(counts.get('Negative', 0) * 100, 1)
    })

# Apply to each asset (row)
direction_summary = comparison_labels_t.apply(summarize_row, axis=1)

# Step 14: Combine label columns with summary columns
final_result = pd.concat([comparison_labels_t, direction_summary], axis=1)

# Step 15: Display
print("\n📊 Asset-Level Direction Summary with Labels:\n")
import IPython
IPython.display.display(final_result)

"""# Future Months Comparison"""

# Ensure index is datetime and sorted
df_aligned.index = pd.to_datetime(df_aligned.index)
df_aligned = df_aligned.sort_index()

# Step 7: Select top N similar months
top_n = 5
top_matches = similarity_df.sort_values(by='Cosine Similarity', ascending=False).head(top_n)
top_match_dates = pd.to_datetime(top_matches.index.tolist())

# Include the target month at the top
all_base_dates = [pd.to_datetime(target_month)] + list(top_match_dates)

# Step 8: Build T and T+1 using row positions
data_dict = {}

for base_date in all_base_dates:
    t_label = base_date.strftime('%Y-%m-%d')

    # Get T (base date) returns
    if base_date in df_aligned.index:
        data_dict[t_label] = df_aligned.loc[base_date]

        # Get T+1 using next index row
        idx_pos = df_aligned.index.get_loc(base_date)
        if idx_pos + 1 < len(df_aligned):
            t1_date = df_aligned.index[idx_pos + 1]
            t1_label = t_label + ' (T+1)'
            data_dict[t1_label] = df_aligned.iloc[idx_pos + 1]
        else:
            print(f"[⚠️] No next month found after {t_label}")
    else:
        print(f"[⚠️] {t_label} not found in df_aligned index.")

# Step 9: Build final display table
comparison_t_t1_df = pd.DataFrame(data_dict).T.round(4).T  # assets as rows

# Step 10: Display
import IPython
print("\n MoM Returns for T and T+1 (next row method):\n")
IPython.display.display(comparison_t_t1_df)

# Step 11: Define labeling function
threshold = 0.002  # 0.2% cutoff

def label_return(x):
    if x > threshold:
        return 'Positive'
    elif x < -threshold:
        return 'Negative'
    else:
        return 'Flat'

# Step 12: Apply labeling function
labeled_df = comparison_t_t1_df.applymap(label_return)

# Step 11: Define labeling function
threshold = 0.002  # 0.2%

def label_return(x):
    if x > threshold:
        return 'Positive'
    elif x < -threshold:
        return 'Negative'
    else:
        return 'Flat'

# Step 12: Apply label to each cell
labeled_df = comparison_t_t1_df.applymap(label_return)

# Step 13: Filter only T+1 columns for summary
t1_columns = [col for col in labeled_df.columns if '(T+1)' in col]
labeled_t1_only = labeled_df[t1_columns]

# Step 14: Calculate % Positive / Flat / Negative from T+1 only
def summarize_labels(row):
    counts = row.value_counts(normalize=True)
    return pd.Series({
        '% Positive': round(counts.get('Positive', 0) * 100, 1),
        '% Flat': round(counts.get('Flat', 0) * 100, 1),
        '% Negative': round(counts.get('Negative', 0) * 100, 1)
    })

label_summary = labeled_t1_only.apply(summarize_labels, axis=1)

# Step 15: Combine full labeled data with T+1 summary stats
final_labeled_table = pd.concat([labeled_df, label_summary], axis=1)

# Step 16: Display
print("\n T & T+1 Direction Labels (Summary Based Only on T+1):\n")
import IPython
IPython.display.display(final_labeled_table)

# Extract only the final summary columns
direction_summary_only = final_labeled_table[['% Positive', '% Flat', '% Negative']]

# Optional: sort alphabetically or by % Positive
# direction_summary_only = direction_summary_only.sort_values(by='% Positive', ascending=False)

# Display the compact summary table
print("\n📊 Summary of T+1 Directional Movement (%):\n")
import IPython
IPython.display.display(direction_summary_only)